# -*- coding: utf-8 -*-
"""llm_model_AI_ML_Lab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/153h21jcriM_KuQ-S81ji5Qx1_MolZb90
"""

!pip install -q -U sentence-transformers

"""# **1.** Loading the dataset"""

import pandas as pd

df = pd.read_csv("/content/drive/MyDrive/Lambton College_AI/updated_combined_data_with_categories_embedded.csv")
df.drop(columns=["Unnamed: 0"], inplace=True)
df = df.head(5000)
df.drop(columns=["combined_text"], inplace=True)
df.head(5)

# Update the combined_text column dynamically using the exact column names
df["combined_text"] = df.apply(
    lambda row: ", ".join([f"{col}: {row[col]}" for col in df.columns if col != "combined_text"]),
    axis=1
)

# Display a sample of the updated column
df.head()

"""# Text embedding with Sentence Transformer."""

from google.colab import drive
drive.mount('/content/drive')

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch


model_path = '/content/drive/(...)/llama2_model' # when the model is saved or in cloud or locally.
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(model_path, device_map="cpu")



"""# Embedding"""

from sentence_transformers import SentenceTransformer

model_path = '/content/drive/(...))/model_st'
model_st = SentenceTransformer(model_path)

# Embedding the "combined text" column
texts = df["combined_text"].tolist()
embeddings = model_st.encode(texts)  # Using SentenceTransformer

# Creating the data_index dictionary
data_index = {
    "texts": texts,
    "embeddings": embeddings
}

# Save data_index to a pickle file
with open('/content/drive/(...)/data_index.pickle', 'wb') as f:
    pickle.dump(data_index, f)

"""A better idea is to save in  a vector database, but I'd like to try using pickle."""

# Loading data_index
import pickle
with open('/content/drive/MyDrive/Lambton College_AI/data_index.pickle', 'rb') as f:
    data_index = pickle.load(f)

# User Query (example)
user_query = "dairy-free meals"
query_embedding = model_st.encode([user_query])[0]

"""# Cosine similarity"""

from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

def similarity_cosine(query_embedding, data_index, top_k=1000):
  ''' Calculate the cosine similarity between the query embedding and all the embeddings in the data_index.

      Args:
        query_embedding: The embedding of the query.
        data_index: A dictionary containing the texts and their corresponding embeddings.
        top_k: The number of most similar texts to retrieve.

      Returns:
        A list of the top_k most similar texts.
  '''
  query_embedding = query_embedding.reshape(1, -1)
  similarities = cosine_similarity(data_index["embeddings"], query_embedding)

  similarities = similarities.flatten()
  indices = np.argsort(similarities)[::-1][:top_k]
  return [data_index["texts"][i] for i in indices]

relevant_texts = similarity_cosine(query_embedding, data_index)

# Generating the prompt
prompt = ""
for text in relevant_texts:
    prompt += f"- {text}\n"

# Changing the format to bulleted list to dataframe
texts = [line[2:].strip() for line in prompt.split('\n') if line.startswith('-')]

print(texts)